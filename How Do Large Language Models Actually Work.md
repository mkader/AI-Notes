 Let’s break it down, step-by-step:

🔹 1. Tokenization :  First, the model slices language into "tokens"—like words or parts of words—turning human input into something it can read.

🔹 2. Embedding :  Tokens become vectors—mathematical representations that let the model understand meaning and context.

🔹 3. Attention :  The model decides what to focus on. Using “self-attention,” it learns which words relate to each other, across the entire sequence.

🔹 4. Feed-Forward Layers :  Each token’s meaning gets refined through dense layers that introduce depth and non-linear understanding.

🔹 5. Normalisation :  Layer norms + residuals keep everything stable while Dropout prevents overfitting. Clean, efficient learning.

🔹 6. Prediction :  Finally, the model generates output by assigning probabilities to possible next tokens. Softmax, temperature, and sampling strategies come into play.

⚡ So, what sets LLMs apart?

 They generate responses step-by-step, considering everything that came before—like a conversation with memory.

💭 In your opinion: Which step contributes most to model hallucinations?

<img src="https://media.licdn.com/dms/image/v2/D4D22AQGTOlMbCArYvQ/feedshare-shrink_1280/B4DZYQd_uaHIAs-/0/1744033023383?e=1746662400&v=beta&t=GNNWDYzQsAmAoSZ7bYnW9q058vXYtds5MYMP0IAk7JA">
