* Most AI models are trained using supervised fine-tuning, meaning they learn by mimicking large datasets of human-annotated examples. This method has limitations.
* DeepSeek R1 overcomes these limitations by using Group Relative Policy Optimization (GRPO), a reinforcement learning technique that improves reasoning efficiency by comparing multiple possible answers within the same context.
* Some facts about DeepSeekâ€™s R1 model are as follows:
  1. DeepSeek-R1 uses a Mixture-of-Experts (MoE) architecture with 671 billion total parameters, activating only 37 billion parameters per task.
  2. It employs selective parameter activation through MoE for resource optimization.
  1. The model is pre-trained on 14.8 trillion tokens across 52 languages.
  1. DeepSeek-R1 was trained using just 2000 Nvidia GPUs. By comparison, ChatGPT-4 needed approximately 25K Nvidia GPUs over 90-100 days.
  1. The model is 85-90% more cost-effective than competitors.
  1. It excels in mathematics, coding, and reasoning tasks.
  1. Also, the model has been released as open-source under the MIT license.
 
<img src="https://substack-post-media.s3.amazonaws.com/public/images/3643f9c2-ad4c-4252-b9f5-6cb733e5a123_1283x1536.jpeg">
